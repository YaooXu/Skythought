

import random
from datasets import load_dataset
import json

import matplotlib.pyplot as plt
import numpy as np
import tqdm
from transformers import AutoTokenizer
from pathlib import Path
import copy
import os
import re

os.environ['HF_HOME'] = '/mnt/workspace/user/sunwangtao/.cache/huggingface'

def replace_thought_content(text):
    return re.sub(
        r"<\|begin_of_thought\|>.*?<\|end_of_thought\|>",
        "<|begin_of_thought|>\n\n<|end_of_thought|>",
        text,
        flags=re.DOTALL
    )

def set_seed(seed):
    random.seed(seed)         
    np.random.seed(seed)      

set_seed(42)

ds = load_dataset("open-thoughts/OpenThoughts-114k", "default")['train']

# åŠ è½½ Qwen/Qwen2.5-7B-Instruct çš„ tokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨æ¥å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„ token é•¿åº¦
token_lengths = []

# éå†æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬

math_code_samples = []

n_math, n_code = 0, 0
for sample in tqdm.tqdm(ds):
    if sample['conversations'][0]['value'].startswith("Return your final response"):
        math_code_samples.append(sample)
        n_math += 1
    # elif sample['conversations'][0]['value'].startswith("Generate an executable Python function"):
    #     math_code_samples.append(sample)
    #     n_code += 1

print(n_math, n_code)

max_length = 16384

random.shuffle(math_code_samples)

long_cot_samples = []
short_cot_samples = []

for sample in tqdm.tqdm(math_code_samples):
    # æ‹¼æ¥æ‰€æœ‰å¯¹è¯ä¸­çš„ value å†…å®¹
    combined_text = sample['conversations'][0]['value'] + sample['conversations'][-1]['value']
    # å¯¹æ‹¼æ¥åçš„æ–‡æœ¬è¿›è¡Œç¼–ç 
    encoded = tokenizer(combined_text, return_tensors='pt', truncation=True, padding=True)
    # è·å– token é•¿åº¦
    token_length = encoded['input_ids'].shape[1]
    # å°† token é•¿åº¦æ·»åŠ åˆ°åˆ—è¡¨ä¸­

    if token_length + 32 <= max_length:
        long_cot_samples.append(sample)

        short_sample = copy.deepcopy(sample)
        short_sample['conversations'][-1]['value'] = replace_thought_content(sample['conversations'][-1]['value'])

        short_cot_samples.append(short_sample)

    if len(long_cot_samples) == 20_000:
        break

long_filename = "skythought/train/LLaMA-Factory/data/Open-Thoughts/math_long_cot_samples-20k.json"
short_filename = "skythought/train/LLaMA-Factory/data/Open-Thoughts/math_short_cot_samples-20k.json"

Path(long_filename).parent.mkdir(parents=True, exist_ok=True)

with open(long_filename, 'w', encoding='utf-8') as f:
    json.dump(long_cot_samples, f, ensure_ascii=False, indent=4)

with open(short_filename, 'w', encoding='utf-8') as f:
    json.dump(short_cot_samples, f, ensure_ascii=False, indent=4)


# # åˆ†åˆ«ç»Ÿè®¡ token æ•°é‡
# long_lengths = []
# short_lengths = []

# for s in tqdm.tqdm(long_cot_samples):
#     combined_text = s['conversations'][0]['value'] + s['conversations'][-1]['value']
#     encoded = tokenizer(combined_text, return_tensors='pt', truncation=True, padding=True)
#     long_lengths.append(encoded['input_ids'].shape[1])

# for s in tqdm.tqdm(short_cot_samples):
#     combined_text = s['conversations'][0]['value'] + s['conversations'][-1]['value']
#     encoded = tokenizer(combined_text, return_tensors='pt', truncation=True, padding=True)
#     short_lengths.append(encoded['input_ids'].shape[1])

# # å¯è§†åŒ–
# plt.figure(figsize=(10, 6))
# plt.hist(long_lengths, bins=50, alpha=0.6, label='Long CoT', color='blue')
# plt.hist(short_lengths, bins=50, alpha=0.6, label='Short CoT', color='orange')
# plt.xlabel("Token Length")
# plt.ylabel("Frequency")
# plt.title("Token Length Distribution: Long CoT vs Short CoT")
# plt.legend()
# plt.grid(True)
# plt.tight_layout()
# plt.show()

# # æ‰“å°åŸºæœ¬ç»Ÿè®¡
# print(f"ğŸ“Š Long CoT å¹³å‡é•¿åº¦: {np.mean(long_lengths):.2f} Â± {np.std(long_lengths):.2f}")
# print(f"ğŸ“Š Short CoT å¹³å‡é•¿åº¦: {np.mean(short_lengths):.2f} Â± {np.std(short_lengths):.2f}")
